import codecs
import os
import sys
import glob
import json
import numpy as np
import scipy as scp
import random
import joblib
import csv
from pathlib import Path
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_selection import SelectFromModel
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC,SVC
from sklearn.neural_network import MLPClassifier
RANDOM_SEED = 50
def set_random():
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)

# Only required for training
INPUT_DIR = 'static/merged'
DATASET_DIR = {'strings':"strings_dataset"}
def save_string_dataset():
    set_random()
    files = glob.glob(os.path.join(INPUT_DIR,'*'))
    names = {}
    content = []
    y = []

    for file in files:
        report_json = json.load(codecs.open(file,'r'))
        
        name = report_json['name']
        strings_file = codecs.open(report_json['strings_file'],'r',encoding='utf-8',errors='ignore')
        strings = [i.strip() for i in strings_file.readlines() if len(i)>3]
        label = report_json['class_label']
        
        names[name] = len(names)
        content.append(' '.join(strings))
        y.append(label)


    print('Fitting Hashing Vectorizer')
    vectorizer = HashingVectorizer(lowercase=False,dtype=np.float32,n_features=2**18)
    X = vectorizer.fit_transform(content)
    y = np.array(y)
    print('X:',X.shape)
    print('y:',y.shape)

    # Create directory for saving strings dataset and models
    Path(DATASET_DIR['strings']).mkdir(parents=True, exist_ok=True)
    print('Saving Dataset')
    np.savez(os.path.join(DATASET_DIR['strings'],'dataset.npz'), X = X, y=y)
    json.dump(names,codecs.open(os.path.join(DATASET_DIR['strings'],'names.json'),'w'))

    print('Saving Vectorizer')
    joblib.dump(vectorizer,os.path.join(DATASET_DIR['strings'],'hvectorizer.pkl'))



def load_string_dataset(f_data=True,f_names=True,f_vectorizer=True):
    set_random()
    names = None
    vectorizer = None
    X,y=None,None
    # Load names
    if f_names:
        names = json.load(codecs.open(os.path.join(DATASET_DIR['strings'],'names.json'),'r'))
    
    # Load hashing vectorizer
    if f_vectorizer:
        vectorizer = joblib.load(os.path.join(DATASET_DIR['strings'],'hvectorizer.pkl'))

    # Load dataset
    if f_data:
        data = np.load(os.path.join(DATASET_DIR['strings'],'dataset.npz'),allow_pickle=True)
        X,y = data['X'],data['y']
        X = X.tolist()
    
    return names,X,y,vectorizer

def get_data_from_list(X,y,names,list_file):
    current_names = {}
    Xres = []
    yres = []
    with open(list_file, newline='') as f:
        reader = csv.reader(f)
        training_data = list(reader)
    for file in training_data:
        name = file[0]
        if name in names:
            current_names[name] = len(current_names)
            Xres.extend(X.getrow(names[name]))
            yres.append(y[names[name]])

    Xres = scp.sparse.vstack(Xres)
    return Xres, yres, current_names
             
def train():
    set_random()
    names,X,y,_ = load_string_dataset() 
    
    Xtrain, ytrain, namestrain = get_data_from_list(X, y, names, 'training_data.csv')

    clf = Pipeline([
        
        ('feature_selection', SelectFromModel(RandomForestClassifier(n_estimators=5000,n_jobs=-1),max_features=2**11)),
        ('classification', VotingClassifier(estimators=[('MLP',MLPClassifier(hidden_layer_sizes= (1024,64,8),verbose=1,batch_size=128,max_iter=2000,activation='relu')),
#                                             ('LSVC',LinearSVC(max_iter=10000)),
                                            ('LR',LogisticRegression(max_iter=10000,n_jobs=-1)),
                                            ('RF',RandomForestClassifier(n_estimators=500,n_jobs=-1)),
                                            ('AdaB',AdaBoostClassifier(n_estimators=500)),                   
                                                        ('GB',GradientBoostingClassifier(n_estimators=500))],voting='soft'))])
    clf = MLPClassifier(hidden_layer_sizes= (1024,64,8),verbose=1,batch_size=128,max_iter=2000,activation='relu')
    print('Training model')    
    clf.fit(Xtrain,ytrain)
    joblib.dump(clf,os.path.join(DATASET_DIR['strings'],'mlp.pkl'))
    
    return clf
    
def validation(clf,list_file='validation_data.csv'):
    set_random()
    names,X,y,_ = load_string_dataset(f_vectorizer=False) 
    Xval, yval , namesval = get_data_from_list(X, y, names, list_file)
    ypred = clf.predict(Xval)
    print('Confusion Matrix')
    print(confusion_matrix(yval,ypred))
    print(classification_report(yval,ypred))
    
def test(clf,file_path):
    set_random()
    _,_,_,vectorizer = load_string_dataset(f_data=False,f_names=False) 
    strings_file = codecs.open(file_path,'r',encoding='utf-8',errors='ignore')
    strings = [i.strip() for i in strings_file.readlines() if len(i)>3]
    x = [' '.join(strings)]
    x = vectorizer.transform(x)
    ypred = clf.predict(x)
    return ypred
    
# save_string_dataset()
# clf = train()
# validation(clf)
# clf = joblib.load(os.path.join(DATASET_DIR['strings'],'vote.pkl'))
# print(test(clf,'/mnt/D/C3iHackathonDataset/static/Benign/0a03518572dc52acf100ce877e32b26f14efca3582779770594232807c0d76b7/String.txt'))


